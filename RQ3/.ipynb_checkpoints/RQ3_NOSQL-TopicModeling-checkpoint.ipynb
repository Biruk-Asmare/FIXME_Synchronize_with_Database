{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import libraries</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim import corpora, models\n",
    "import pyLDAvis.gensim\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from itertools import chain\n",
    "import string\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import math\n",
    "import pickle\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>NOSQL Dataset</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsql_df=pd.read_csv(\"../Data_Collection/NOSQL_raw_dataset.csv\")\n",
    "nsql_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicate comments from the dataset\n",
    "nsdata=nsql_df[nsql_df[\"is_dac\"]==True] #select only data-access SATD\n",
    "nsdata = nsdata.sort_values('version', ascending=False)\n",
    "nsdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsdata = nsdata.drop_duplicates(subset=\"comment\", keep='first')\n",
    "nsdata.info()\n",
    "nsdata.to_csv(\"DAC_NOSQL_dataset_NoDuplicates.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Cleaning </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words= set(stopwords.words('english'))\n",
    "stop_words.add(\"todo\") #too common\n",
    "stop_words.add(\"fixme\") #too common\n",
    "remove= set(string.punctuation)\n",
    "stemmer= SnowballStemmer(language=\"english\")\n",
    "def lemmatize_and_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def get_cleaned_list(text):\n",
    "    re_stop_word=' '.join([word for word in text.lower().split() if word not in stop_words])\n",
    "    re_punc=''.join(c for c in re_stop_word if c not in remove)\n",
    "    #stem_text=' '.join([stemmer.stem (token) for token in re_punc.split()])\n",
    "    lema_text=' '.join([lemmatize_and_stemming (token) for token in re_punc.split()])\n",
    "    return lema_text.split()\n",
    "    #return stem_text.split() #uncomment for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsdata['clean_comments']= nsdata.comment.apply(get_cleaned_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nsdata.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LDA with TF-IDF</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary\n",
    "docs=nsdata['clean_comments'].tolist()\n",
    "dictionary = gensim.corpora.Dictionary(docs)\n",
    "len(dictionary)\n",
    "#len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create BOW for each doc\n",
    "bow=[dictionary.doc2bow(doc) for doc in docs]\n",
    "len(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create TF-IDF model\n",
    "tfidf = models.TfidfModel(bow)\n",
    "docs_tfidf=tfidf[bow]\n",
    "len(docs_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets find optimum topics passes=1\n",
    "for topic in range(5,150,5):\n",
    "\n",
    "    lda_model = gensim.models.LdaMulticore(docs_tfidf, num_topics=topic, passes=1,random_state=100, id2word=dictionary, workers=6)\n",
    "    coherence_model= CoherenceModel(model=lda_model, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "    coherence= coherence_model.get_coherence()\n",
    "    print(\"{},{} \\n\".format(topic,coherence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets find optimum passes topics=100\n",
    "for _pass in range(1,20):\n",
    "    lda_model = gensim.models.LdaMulticore(docs_tfidf, num_topics=100, passes=_pass,random_state=100, id2word=dictionary, workers=6)\n",
    "    coherence_model= CoherenceModel(model=lda_model, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "    coherence= coherence_model.get_coherence()\n",
    "    print(\"{},{} \\n\".format(_pass,coherence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#50, 1pass is the number of topic with the highest score\n",
    "#lets find optimum beta topics=125, passes=1\n",
    "betas = list(np.arange(0.01, 1, 0.3))\n",
    "betas.append('symmetric')\n",
    "for beta in betas:\n",
    "    lda_model = gensim.models.LdaMulticore(docs_tfidf, num_topics=100, passes=1,eta=beta,random_state=100, id2word=dictionary, workers=6)\n",
    "    coherence_model= CoherenceModel(model=lda_model, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "    coherence= coherence_model.get_coherence()\n",
    "    print(\"{},{} \\n\".format(beta,coherence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#50, 1 pass is the number of topic with the highest score, beta does not matter\n",
    "#lets find optimum alpha topics=125, passes=1\n",
    "alphas = list(np.arange(0.01, 1, 0.3))\n",
    "alphas.append('symmetric')\n",
    "alphas.append('asymmetric')\n",
    "for alpha in alphas:\n",
    "    lda_model = gensim.models.LdaMulticore(docs_tfidf, num_topics=100, passes=1,alpha=alpha,random_state=100, id2word=dictionary, workers=6)\n",
    "    coherence_model= CoherenceModel(model=lda_model, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "    coherence= coherence_model.get_coherence()\n",
    "    print(\"{},{} \\n\".format(alpha,coherence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final model\n",
    "lda_model = gensim.models.LdaMulticore(docs_tfidf, num_topics=4, passes=1,random_state=100, id2word=dictionary, workers=6)\n",
    "coherence_model= CoherenceModel(model=lda_model, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence= coherence_model.get_coherence()\n",
    "print(\"Coherence score: {} \".format(coherence))\n",
    "#save the model as pkl\n",
    "lda_model.save(\"NOSqlTopicModel.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now classsify the comments, based on the topic\n",
    "results=[]\n",
    "\n",
    "for b in bow:\n",
    "    res=lda_model.get_document_topics(b, minimum_probability=0)\n",
    "    #print(\"{:.60f}\".format(row[1]))\n",
    "    topic=max(res, key=lambda x: x[1])\n",
    "    print(topic)\n",
    "    results.append(topic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsdata[\"topic\"]=results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsdata.groupby(['topic']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsdata[nsdata[\"topic\"]==0].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the classified data as csv\n",
    "nsdata.to_csv(\"DAC_NOSQL_dataset_NoDuplicates_Classified.csv\", index=False)\n",
    "nsdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsdata.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
